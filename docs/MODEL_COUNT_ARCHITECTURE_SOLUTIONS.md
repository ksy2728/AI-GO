# 📈 AI-GO 모델 수 추적 아키텍처 개선 보고서

## 문제 요약
- **데이터 소스 분산 및 비일관성**: 모델 수 집계에 여러 데이터 소스(예: GitHub 데이터, 데이터베이스, 임시 데이터 등)가 사용되면서 환경별로 다른 결과를 보여주고 있음【16†L75-L83】. 예를 들어 로컬 환경은 실제 DB 기반 *63개* 모델 중 *51개 활성*으로 표시되나, 배포 환경에서는 전역 하드코딩 값 *139개* 중 *128개 활성*으로 표시되는 불일치가 발생함【33†L1-L4】【33†L37-L45】.
- **하드코딩된 전역 모델 값 사용**: 배포 환경에서 실시간 통계 API(`/api/v1/realtime-stats`)가 실제 데이터 대신 *139*처럼 하드코딩된 전역 모델 수를 사용하고 랜덤 보정만 수행하여 실제와 동떨어진 값을 제공함【33†L23-L31】【33†L41-L48】. 이로 인해 정확한 실시간 모델 수 추적이 불가능하고 환경 간 지표 차이가 발생함.
- **상태 동기화 지연**: 모델 활성/비활성 상태(operational 등) 업데이트가 실시간으로 반영되지 않고, 일정 주기 데이터(예: 최근 30분 내 상태) 기반으로 집계되어 **상태 변경 시점과 표시 간에 지연**이 존재함【26†L63-L72】【26†L75-L83】. 또한 캐시를 사용하면서 (예: Redis 30초 TTL) 짧은 시간 동안 클라이언트에 구식 정보가 남을 수 있음【37†L83-L92】.
- **WebSocket 비활성화**: 현재 아키텍처에서 WebSocket 기반 실시간 업데이트가 환경 설정으로 비활성화되어 있으며, 대신 주기적인 REST API 폴링으로 대체되고 있음【22†L37-L45】. Vercel 배포 환경 제약으로 인한 SSE/대안 미도입으로 **실시간 푸시가 부재**한 상태임.
- **병목 및 효율성 문제**: 모델 수 집계 로직이 GitHub API 호출(수 초 소요)이나 데이터베이스 풀 스캔 등에 의존하여 **응답 지연**이 발생함【16†L77-L84】. 예컨대 GitHub 데이터 소스는 2~5초 응답 시간이 걸려 실시간 집계에는 부적합하고, 전체 모델 목록을 가져와 필터링하는 비효율이 존재함【16†L77-L84】【33†L1-L4】. 
- **데이터 흐름 레이스 컨디션 가능성**: 다중 소스와 캐시 계층을 거치는 현재 데이터 흐름에서, 업데이트 순서나 지연에 따라 **일시적인 불일치(race condition)**가 발생할 수 있음. 예를 들어 새로운 모델 추가 시 DB에는 즉시 반영되지만 Redis 캐시는 만료 전까지 이전 숫자를 유지하거나, 지역 레플리카 DB와 본 DB 간 복제 지연으로 지역별 조회 시점에 차이가 날 수 있음.
- **다국어/다지역 지원과 정확성 균형**: 플랫폼이 다국어 UI와 다지역 인프라를 지원하면서, **지역별로 모델 가용성 차이** 또는 **언어별 데이터 경합** 이슈가 고려되어야 함. 예를 들어 전역 DB에 신규 모델이 추가될 때 각 지역(Read Replica)이 이를 동기화하는 데 지연이 있을 수 있으며, 사용자 언어에 따라 필터링된 뷰를 제공할 경우 총 모델 수 인지가 달라질 수 있음. 현재 설계에서 모델 이름/설명은 JSON 다국어 필드를 사용하지만【19†L59-L67】, 모델 카운트는 전역 기준으로 일관성을 유지해야 하므로 이를 위한 특별한 처리 또는 지연 허용 범위 설정이 필요함.

## 원인 분석
- **분산된 데이터 소스 설계**: 개발 초기 편의와 장애 대비를 위해 GitHub 정적 데이터, TempData 폴백, 로컬 DB 등 **여러 소스에 걸친 데이터 파이프라인**을 구성한 것이 근본 원인이다【16†L75-L83】. 환경에 따라 우선 사용 소스가 달라(예: 배포환경은 TempData→GitHub, 개발환경은 DB→GitHub) 서로 다른 모델 수를 산출하게 되었고, 단일 진실(Single Source of Truth)이 없었다【33†L43-L47】. 또한 이러한 다중 소스 로직이 시간 경과하면서 일부 경로(예: GitHub 동기화)에서는 최신 데이터가 반영되지 않거나 임시 데이터가 갱신되지 않는 기술부채가 쌓였다.
- **글로벌 모델 수 하드코딩**: 서버리스 환경(Vercel)의 제약과 실시간 집계 구현 난이도로 인해, 배포 버전에서는 우선 "전세계 모델 총량"을 임의 값으로 고정하는 임시방편을 택했다【33†L21-L29】. 이는 빠른 응답을 얻으려는 의도였으나 실제 값과 무관한 숫자를 노출하여 데이터 신뢰성을 훼손했다. 랜덤 ±3 변동까지 줘서 그럴듯하게 보이도록 한 것은 사용자를 혼동시킬 뿐이며, 이후 기능 확장(모델 추가/제거)에 전혀 대응할 수 없는 구조적인 한계로 드러났다【33†L25-L33】.
- **웹소켓 비활성화의 구조적 제약**: `useRealtime` 훅에서 **프로덕션 모드일 때 WebSocket을 아예 끄도록 설정**한 것은 Vercel에서 지속 연결을 지원하지 않기 때문이며【22†L37-L45】, 대안 구현(SSE 등)을 마련하지 않은 상태에서 실시간 기능을 사실상 포기한 원인이다. 그 결과 클라이언트는 항상 REST API 폴백(`useRealtime`훅이 null 반환)으로 동작하고, 실시간 업데이트가 필요한 상황에서도 **주기적 재조회** 외 방법이 없다【22†L49-L57】. 이는 신속한 상태 반영이 어렵고, 사용자가 여러 번 새로고침해야 최신 정보를 얻는 비효율을 낳았다.
- **비효율적 집계 연산**: 현재 모델 수 집계는 모든 모델 목록을 불러와 프론트엔드에서 필터링하거나, DB에서 count와 groupBy 쿼리를 매번 실행하는 식이다【26†L55-L63】【26†L81-L90】. 모델 수 자체는 비교적 적지만(수십~수백개) 추후 확장 시 **매 요청마다 전체를 읽는 방식은 확장성에 한계**가 있다. 또한 활성/비활성 모델 수를 계산하기 위해 최근 상태 기록을 groupBy하는 쿼리는 시간창이 커질수록 느려질 수 있다【26†L63-L72】【26†L81-L90】. 캐시를 도입했으나 캐시 적중률이 낮거나 만료 주기가 짧으면 오히려 반복 연산이 많아져 병목이 된다.
- **캐싱 및 동기화 전략 한계**: Redis를 활용한 시스템 캐시(`system:stats` 키 등)는 2분 등 제한된 기간만 유효해 만료 시 대량 DB 쿼리가 다시 발생하며【26†L109-L118】, 여러 웹서버 인스턴스 간 캐시 미스 타이밍이 겹치면 동시에 비슷한 쿼리를 유발할 수 있다. 또한 **캐시 갱신 트리거 부재**로 인해 새로운 모델 추가 직후에도 최대 TTL 만큼 반영이 늦어질 수 있다. 코드 상에서 캐시를 무효화하거나 업데이트하는 Pub/Sub 매커니즘이 보이지 않아(예: `cache:invalidate:models` 채널 활용 미흡), 수동 만료에 의존하고 있다.
- **데이터 흐름 Race Condition**: 프론트엔드에서 `ModelsContext` (실제 모델 목록 기반)와 `realtime-stats API` (하드코드 기반)를 혼용하면서, 둘 간의 **타이밍 차이에 따른 불일치**가 발생할 수 있다. 예를 들어 어떤 렌더링 시점에는 Context에서 가져온 `totalModels` (63개)이 사용되다가 곧 API에서 받아온 `139개`로 덮어써지는 식의 일시적 잘못된 표시가 가능하다. 다행히 코드에서 `effectiveStats = globalStats || contextStats`로 우선순위를 정하고 있지만【16†L145-L153】, 그럼에도 UI 표시에는 여전히 `|| 139`가 남아있어 논리적 충돌이 있었다【17†L160-L168】. 이러한 설계는 동시 업데이트 상황에서 예측하기 어려운 버그를 일으킬 소지가 있다.
- **다지역 데이터 동기화 문제**: 글로벌 아키텍처 상, 모든 모델/프로바이더 데이터는 미주 본서버에서 작성되고 각 지역 리플리카로 복제되는데【32†L47-L56】, 이 복제에 수 초 이상의 지연이 생길 경우 지역 API에서 조회한 모델 수가 본서버와 달라질 수 있다. 현재는 비교적 모델 추가 빈도가 낮아 큰 문제는 없지만, 향후 여러 지역에서 동시 서비스되는 환경에서는 **일관성 vs 지연** 사이에 트레이드오프가 발생한다. 예컨대 업데이트 직후 짧은 기간 동안 지역별 표시값이 다를 수 있으며, 이를 감지하거나 사용자에게 알릴 로직도 필요할 것이다. 다국어 지원의 경우, 모델 명/설명 등의 현지화가 모델 수 자체에는 영향을 주지 않으나, 지역별 가용 모델 종류(예: 특정 모델이 일부 지역 규제로 제외되는 경우 등)를 다르게 취급해야 할 시 모델 "총 수"의 정의를 전역/지역 중 무엇으로 할지 정책 결정이 필요하다.

## 아키텍처 제안
위 문제들을 해결하고 **artificialanalysis.ai 플랫폼 수준의 실시간 모델 추적**을 지원하기 위해, 다음과 같은 아키텍처 개선 방안을 제안합니다:

### 1. 단일 진실 원천 및 Pub/Sub 이벤트 흐름
현재처럼 다중 데이터 소스에 의존하지 말고, **단일 데이터 계층(Single Source of Truth)**을 확립합니다. PostgreSQL(DB)을 중심으로 하되, 모델 추가/상태 변경 등의 이벤트를 발생시켜 실시간 동기화합니다. 예를 들어:
- **이벤트 발행(Publish)**: 새로운 모델이 추가되거나 상태 값(`is_active` 또는 상태 테이블)이 변경되면, 해당 서비스(예: Admin API나 백엔드 배치)가 Redis Pub/Sub 채널에 이벤트 메시지를 발행합니다. 메시지에는 모델 ID, 변경 내용, 타임스탬프 등이 포함됩니다. 
- **이벤트 구독(Subscribe)**: 웹소켓/SSE 서버 및 캐시 서비스가 이 채널을 구독하여 이벤트 수신 시 즉시 내부 데이터를 갱신하거나 클라이언트로 푸시합니다. 
- **실시간 업데이트**: 구독자로부터 이벤트를 받은 WebSocket/SSE 레이어는 연결된 클라이언트들에게 **모델 수 변동**이나 **상태 변화**를 즉시 전송합니다. 또한 Redis에 저장된 캐시 값을 해당 이벤트 기반으로 갱신/무효화하여 이후 API 요청에도 반영되도록 합니다 (예: `PUBLISH cache:invalidate:models "<modelId>"` 형태로 수신 시 `system:stats` 캐시 삭제)【21†L381-L389】.

이러한 Pub/Sub 기반 아키텍처에서는 **폴링 없이 푸시**로 데이터가 동기화되므로 지연이 최소화되고, 하나의 DB 원본을 바라보면서 변경 이벤트로 동작하기 때문에 데이터 일관성도 향상됩니다. 가령, 한 지역에서 모델이 추가되면 해당 이벤트가 전세계 서비스 인스턴스에 전파되어 모든 사용자가 동시에 갱신된 모델 수를 보게 됩니다. 또한 이벤트 스트림(예: Kafka 같은)으로 확장하면 향후 **모델 추가/삭제의 히스토리 추적**이나 **실시간 분석 처리**(stream processing)도 가능해집니다.

CRDT 기반 접근도 고려할 수 있습니다. 만약 여러 데이터 센터에서 동시 쓰기나 오프라인 카운팅이 필요하다면, **G-Counter**와 같은 CRDT를 활용하여 각 노드의 모델 추가 수를 병합할 수 있습니다. CRDT의 강점은 충돌 없이 최종 일관성을 보장하는 것이므로, 예를 들어 지역 별로 모델 카운트를 집계하다가 주기적으로 병합하는 분산 시나리오에 적용 가능합니다. 그러나 현 상황에서는 단일 DB가 권한을 가지므로 CRDT까지 가지 않아도 되며, 대신 Pub/Sub으로 충분히 eventual consistency를 달성할 수 있습니다.

```mermaid
flowchart LR
    subgraph Backend (Server)
    A[PostgreSQL<br>TimescaleDB] -- New model inserted --> B((Event Bus<br>Redis Pub/Sub))
    B --> C[Cache Layer<br>(Redis/Memory)]
    B --> D((WebSocket/SSE Server))
    end
    subgraph Frontend (Client)
    E[Monitoring Page] -- subscribe --> D
    E -- periodic fetch --> F[/API (fallback)/]
    end
    C -- serve cached count --> F
    A -- direct query (miss) --> F
    D -- push updated count --> E
```

*상기 다이어그램:* 모델 데이터 변경 시 이벤트가 발행되어(`B`) **캐시(`C`)**와 **실시간 채널(`D`)** 모두에 전파되고, 프론트엔드는 WebSocket/SSE 구독으로 즉각 반영(`E`), 또는 폴백으로 API를 통해 캐시된 최신 데이터(`F`)를 가져옵니다.

### 2. PostgreSQL/TimescaleDB 스키마 및 인덱스 최적화
- **실시간 집계를 위한 인덱스**: 모델 수 조회 쿼리는 매우 빈번하므로, 해당 쿼리를 최적화하기 위해 DB 인덱스를 최적화합니다. 현재 `models` 테이블에 활성화 여부(`is_active`) 필드에 인덱스가 존재하여 활성 모델 count 시 활용되고 있습니다【19†L81-L89】. 이를 적극 활용하도록 ORM 쿼리를 구성하고, 추가로 **복합 인덱스** (예: `provider별 활성 모델`) 등이 필요하면 설계합니다. 예를 들어 다지역 통계를 동시에 낸다면 `(region, is_active)` 조합 인덱스를 고려할 수 있습니다.
- **TimescaleDB 활용 여부**: TimescaleDB는 주로 시간 단위 통계에 활용되고 있으며, 5분/1시간 단위의 지속 집계 뷰도 정의되어 있습니다【21†L279-L288】【21†L304-L313】. 그러나 *현재 시점의 모델 수*를 구하는 데에는 굳이 continuous aggregate를 거칠 필요 없이, **PostgreSQL의 실시간 쿼리**가 적합합니다. TimescaleDB 뷰는 최소 1분 단위 지연을 가지므로(continuous aggregate 정책 상) 정확한 실시간 숫자를 위해서는 사용하지 않는 편이 좋습니다. 대신, TimescaleDB에는 과거 **모델 수 추이**를 저장하여 트렌드 그래프를 그리는 용도로 활용할 수 있습니다. 예를 들어 시간별로 active 모델 수를 기록해두고 대시보드에서 히스토리 차트를 제공하면 부가가치가 있습니다. 이때 continuous aggregate 뷰 (예: 5분 bucket 평균 등)를 사용하면 DB 부하 없이 손쉽게 시계열 집계를 보여줄 수 있습니다. 하지만 **현재 시각의 누적값**은 TimescaleDB가 아닌 main DB (Postgres)의 트랜잭션 데이터를 기준으로 하는 것이 정확하고 빠릅니다.
- **통계성 테이블/뷰 추가**: 실시간 요청 부하를 낮추기 위해, **미리 계산된 통계**를 저장/활용하는 방안을 고려합니다. 예를 들어 `model_stats_summary` 같은 뷰 또는 매테리얼라이즈드 뷰를 만들어 `전체 모델 수`와 `활성 모델 수`를 갱신해둘 수 있습니다. 이 뷰는 트리거 또는 스케줄러를 통해 10~30초마다 갱신하도록 하고, API 호출 시엔 해당 뷰의 결과만 조회하면 <10ms 수준으로 응답할 수 있습니다. PostgreSQL의 RULE 또는 REFRESH MATERIALIZED VIEW 기능을 이용하거나, TimescaleDB의 continuous aggregate를 1분 이내 간격으로 돌리는 것도 한 방법입니다. 다만, 트리거 방식은 모델 추가/상태 변경이 매우 자주 일어나면 오버헤드가 될 수 있으므로 현재 규모(하루 변경 몇 건 수준)에서는 유효한 최적화가 될 것입니다.
- **ANALYZE 및 쿼리 플래너 튜닝**: 모델 및 상태 관련 테이블에 정기적으로 `ANALYZE`를 수행하여 통계 정보를 최신으로 유지하면, PostgreSQL이 최적의 실행 계획(예: 인덱스 스캔 vs 시퀀셜 스캔)을 선택하는데 도움이 됩니다. 특히 `modelStatus`처럼 지속적으로 쌓이는 테이블은 row count가 변하므로, 통계가 낡으면 쿼리가 느려질 수 있습니다. TimescaleDB의 하이퍼테이블(`status_probes`)도 동일하게 자동 분석이 되도록 설정하거나 수동 관리가 필요합니다.
- **대용량 대비 파티셔닝**: 향후 모델 수가 수천 개 이상으로 늘어나면 단순 count 쿼리도 부하가 될 수 있습니다. 이를 대비해 `models` 테이블 자체를 지역별로 파티셔닝하거나, 활성/비활성 별 분할하는 것은 과한 최적화일 수 있으나, TimescaleDB를 이미 활용하므로 **최신 상태**는 Redis 캐시 등으로 소화하고, 과거 데이터는 TimescaleDB로 아카이빙하는 구조를 유지하는 것이 좋습니다. 이미 디자인 상 status_probes를 90일 보관, 그 이후 S3로 아카이빙하도록 계획되어 있습니다【37†L75-L78】.

### 3. Prisma ORM 및 API 레이어 경량화
- **불필요한 데이터 로드 제거**: 현재 Prisma 쿼리에서 모델과 연관된 모든 상태 레코드를 join하여 가져온 뒤 자바스크립트 단에서 필터링/집계하는 로직이 일부 존재합니다【26†L139-L148】【26†L153-L162】. 이를 개선하여 **DB에서 필요한 집계를 수행**하도록 쿼리를 최적화합니다. 예를 들어, 공급자별 모델 수와 가용성 평균을 구하는 로직은 아래처럼 단일 쿼리로 대체 가능합니다:
  ```typescript
  const providerStats = await prisma.model.groupBy({
    by: ['providerId'],
    where: { isActive: true },
    _count: { _all: true },
    _avg: { status: { availability: true } }
  })
  ```
  위 쿼리는 Prisma pseudo-code이며, 실제로는 Prisma에서 관계 필드 aggregation이 제한적일 수 있으므로 **raw SQL** 또는 **뷰**를 활용하는 것도 고려합니다. 핵심은, **DB가 더 효율적인 집계 연산은 DB에 위임**하고, API 서버는 최소한의 로직(캐싱 및 직렬화)에 집중하도록 하는 것입니다. 이로써 데이터 전송량과 서버 연산 부하를 줄일 수 있습니다.
- **Lazy Loading 적용**: 상세 상태를 매번 모두 불러오는 대신, 요약 정보 조회에서는 status 세부 필드까지 join하지 않도록 합니다. 예를 들어 `/api/v1/realtime-stats`에서는 전체 모델 객체를 반환하지 않고 { total, active, ... } 숫자들만 반환하도록 최소화합니다【33†L23-L31】. 이미 경량 응답이긴 하나, 내부 구현에서도 `prisma.model.count` 등을 사용하여 객체 생성 오버헤드를 줄였습니다【26†L57-L65】. 이 접근을 확장해, 상세 페이지나 필요 시에만 모델 상세 정보를 조회하고 일반 모니터링에서는 count/percentage만 다루도록 API를 이원화합니다 (예: `GET /v1/stats/summary` vs `GET /v1/models`).
- **Prisma 최적 버전 사용 및 튜닝**: Prisma ORM 자체의 성능도 중요합니다. 대량 데이터 시 메모리 사용을 최적화한 최신 버전을 사용하고, 필요하다면 `batchSize` 조절이나 `findMany` 시 페이지네이션을 적용합니다. 예를 들어, 수백개 모델을 한꺼번에 가져와 프론트로 보내지 말고, 초기 로드에서는 상위 50개 등만 보내고 "더 보기" 시 추가 로딩하는 페이징 UI로 개선할 수 있습니다. 실제 개선안에 이 방향이 논의되었으며, 서버리스 함수 제약으로 초기 로드 모델 수를 줄이는 조정이 있었습니다【6†L46-L54】.
- **API 응답 캐싱 및 압축**: 모델 수 같은 변화가 잦지 않은 데이터는 CDN 캐시나 브라우저 캐시를 활용할 수 있습니다. 글로벌 아키텍처 문서에 나온 바와 같이, 브라우저 5분, CDN 2분, Redis 30초 계층 캐싱을 적용하면 대부분의 요청은 캐시 적중으로 처리될 수 있습니다【37†L83-L92】. 또한 JSON 응답에 GZIP 압축 혹은 HTTP/2 push 사용, 필요한 필드만 포함하여 크기를 최소화하면 네트워크 지연을 줄일 수 있습니다. 
- **GraphQL 및 지속 연결 활용**: 중장기적으로 GraphQL 구독을 도입하면 클라이언트가 필요한 필드만 실시간으로 받을 수 있어 효율적입니다. REST+WebSocket 대비 오버헤드가 크지 않고, subscription으로 `totalModels` 필드만 지정 가능하므로 전송량을 절약합니다. 다만 GraphQL 인프라 추가 도입 비용을 고려해, 우선 REST+SSE 방식으로 구축 후 차차 이전하는 것을 권장합니다.

### 4. 폴링 vs 스트리밍: 실시간 통신 전략
실시간 모델 수 갱신을 위해 **어떤 통신 방식을 채택할지**에 대한 가이드입니다:

- **폴링(Polling)**: 클라이언트가 정해진 주기마다 서버 API를 호출하여 최신 데이터를 가져오는 방식입니다. 구현이 단순하고 기존 REST API를 재사용할 수 있지만, 불필요한 반복 호출로 서버 부하와 응답 지연이 커집니다. 모델 수 변화가 **드물거나 일정 주기로만 발생**한다면 (예: 하루 몇 번 업데이트) 폴링 주기를 길게 가져도 무방합니다. 그러나 자주 변하는 상태(예: 모델 장애 발생 등)를 즉각 반영해야 한다면 최대 5~10초 이내로 폴링 주기를 설정해야 하며, 이는 결국 상시 트래픽 증가로 이어집니다.
- **스트리밍(Streaming)**: 서버가 클라이언트에게 데이터 변화 시 바로 푸시하는 방식으로, WebSocket이나 Server-Sent Events(SSE)가 대표적입니다. **WebSocket**은 양방향 통신이 가능해 클라이언트도 메시지를 보내 상호작용할 수 있지만, 서버리스 환경에서는 직접 지원이 어려워 별도 서버이나 PaaS가 필요합니다. **SSE**는 HTTP 기반 단방향 스트림으로 비교적 구현 난이도가 낮고 Vercel Edge Functions 등에서도 구현 가능하므로, AI-GO의 실시간 업데이트에는 SSE가 적합한 대안입니다【22†L61-L64】. 스트리밍은 데이터 변화가 자주 일어날 때 효율적이며, **이벤트 기반 푸시라 변화가 없으면 네트워크 사용이 0**인 장점이 있습니다. 단, 클라이언트-서버 간 연결을 계속 유지하므로 사용자가 매우 많을 경우 연결 관리에 부하가 생길 수 있습니다 (수만 개 소켓 동시 유지 등).
- **혼합 전략**: 두 방식을 혼용하여 안정성과 실시간성을 모두 잡을 수 있습니다. 예를 들어, 기본적으로 SSE로 푸시를 받되 만약 연결이 끊어지거나 백그라운드에서 SSE 수신이 중단된 경우를 대비해 **주기적 폴링을 백업**으로 둡니다. 또한 초기 페이지 로드시에는 폴링(정적 데이터)을 하고, 페이지가 활성화되면 SSE 연결을 맺는 방식도 가능합니다. 이것은 트래픽을 어느 정도 절감하면서 중요한 업데이트는 실시간으로 받는 절충안입니다.

다음은 폴링과 스트리밍 방식을 비교한 표입니다:

| 기준                     | 폴링 (Polling)                                | 스트리밍 (WebSocket/SSE)                         |
|------------------------|--------------------------------------------|----------------------------------------------|
| **구현 복잡도**          | 낮음 – 기존 REST API 활용, 타이머만 구현            | 중간 – 서버/클라이언트에 연결 관리 로직 필요 (SSE는 비교적 쉬움) |
| **서버 부하**            | 높음 – 모든 클라이언트가 주기적으로 요청, 변화 없을 때도 부하 | 낮음 – 변화 발생 시에만 푸시, idle 시 부하 거의 0      |
| **데이터 신선도(지연)**   | 주기에 의존 – 최악의 경우 `주기`만큼 늦게 반영           | 매우 높음 – 거의 즉시 반영 (초단위 미만 지연)           |
| **스케일 (동접)**        | 매우 높음 – 요청 자체는 짧아 많아도 수평 확장으로 커버 가능    | 중간 – 각 클라이언트와 지속연결 유지, 수만 단위 동접 시 튜닝 필요 |
| **환경 제약**           | 없음 – HTTP 지원 환경이면 동작                   | 있음 – WebSocket은 프록시/방화벽 이슈, SSE는 HTTP/2 권장 |
| **양방향성**            | 불가능 – 클라이언트→서버 전달은 별도 요청 필요           | 가능 – (WebSocket의 경우) 양방향 실시간 통신           |

> **결론**: AI-GO의 실시간 모델 추적에는 **스트리밍 방식이 권장**됩니다. 특히 상태 변화가 즉각 사용자에게 중요하게 전달되어야 하는 서비스이므로, SSE를 통해 몇 초 이내로 업데이트를 반영하고, 폴링은 보조적으로 사용해 안정성을 높이는 전략이 적합합니다. 다만 초기 단계에서는 구현 편의상 10~15초 폴링으로 시작하고, 추후 WebSocket/SSE로 전환하는 방안도 고려할 수 있습니다 (로드맵 상 Phase 3~4에 해당).

### 5. 다국어/다지역 지원과 모델 수 정확성 유지
- **다지역 데이터 일관성**: 앞서 제안한 이벤트 기반 싱크 또는 캐시 무효화 전략을 전지역에 적용하여, 지역별 데이터 지연을 최소화합니다. PostgreSQL Logical Replication을 사용 중이라면【32†L49-L58】 복제 지연 모니터링을 구축하고, 만약 지연이 임계치(예: 5초)를 넘는 경우 해당 지역의 API 요청을 일시적으로 메인 DB로 포워딩하는 방법도 고려합니다. 즉, **지역 DB 지연 감지 → 중앙 DB 조회 폴백** 로직을 넣으면 최악의 경우에도 정확도를 보장할 수 있습니다. 또한 각 지역 캐시 서버도 중앙 이벤트를 구독하므로, 비록 DB 복제가 늦더라도 Redis 캐시를 통해 중앙에서 계산된 최신 모델 수를 받아 표시할 수 있습니다. 이것은 eventual consistency 환경에서 정확성을 높이는 현실적인 방안입니다.
- **다국어 UI와 데이터 처리**: 모델의 이름과 설명 등은 다국어로 제공되지만, **모델의 식별과 카운트는 공통**입니다. 따라서 백엔드 로직은 locale에 상관없이 동일한 모델 ID 세트를 집계하되, 프론트엔드 표시만 현지 언어로 합니다. 이를 위해 API 요청 시 locale 파라미터를 받아 모델 리스트를 필터링하거나 하지 않고, 모든 locale에서 동일한 `totalModels` 값을 주고 클라이언트에서 번역만 적용합니다. GraphQL 도입 시 locale별 필드를 resolver 레벨에서 처리하고, count는 공유 resolver를 사용하면 될 것입니다. 중요한 것은 다국어로 인해 같은 모델이 이중으로 집계되거나 누락되는 일이 없도록 **유일 키(ID)** 기반으로 연산하는 것입니다 (예: `COUNT(DISTINCT model.id)` 형태로 집계하여 혹시 조인으로 중복될 가능성 배제).
- **지역별 모델 가용성 차이**: 만약 향후 국가 정책 등으로 일부 모델이 특정 지역에만 비활성화된다면, "총 모델 수" 정의를 재검토해야 합니다. 선택지는 (a) **전세계 등록된 총 모델 수**를 그대로 두고 지역별 비활성은 별도 지표로 표시하거나, (b) 사용자 위치에 따라 *가용* 모델 수만 보여주는 것입니다. (a)는 일관성 측면에서 좋으나 사용자에게 의미없는 숫자가 될 수 있고, (b)는 정확성은 높으나 지역마다 수치가 달라 혼란을 줄 수 있습니다. 본 분석에서는 (a)안을 채택하되 UI에 "귀하의 지역에서 X개 사용 가능" 등을 보조 표기하는 타협안을 권장합니다. 이렇게 하면 기본적으로 전세계적인 모델 현황 파악은 유지하면서도 지역 제한을 명시할 수 있습니다. 구현적으로는 models 테이블에 region 필드나 `model_endpoints` 테이블의 지역 정보를 활용하여 필터링된 count도 계산할 수 있습니다【19†L89-L98】. 다만 기본 `totalModels`는 region 필터를 무시한 전역 count로 두고, UI에서 별도로 region 필터 count를 병기하는 형태를 제안합니다.

## 우선순위 및 예상 효과
위 제안들의 구현 우선순위와 기대되는 효과를 정리하면 다음과 같습니다:

| 개선 항목                         | 우선순위     | 예상 효과 및 지표 개선                  |
|---------------------------------|----------|-------------------------------------|
| **하드코딩 제거 및 실제 데이터 적용**<br/>(`realtime-stats` API 수정 및 데이터 소스 통일) | 높음 (즉시)【33†L61-L70】 | - 기능 정상화: 로컬/배포 환경 모델 수 일치<br/>- 정확도 100% 달성 (현재 60% → 100% 일관성【23†L289-L297】)<br/>- 신뢰성 향상으로 사용자 혼란 감소 |
| **WebSocket/SSE 실시간 업데이트 도입**<br/>(웹소켓 서버 또는 SSE 엔드포인트 구축) | 높음 (단기)【33†L61-L70】 | - 실시간성 대폭 향상 (지연 30초 → <5초)<br/>- Active 모델 수 변화 모니터링 향상<br/>- 사용자 경험 개선 (즉각적인 상태 반영) |
| **데이터 소스 일원화 및 캐시 연동**<br/>(UnifiedStatsService 구현, 이벤트 기반 캐시 무효화) | 높음 (단기)【33†L67-L70】 | - 데이터 일관성 95% 이상 보장 (캐시 stale 최소화)<br/>- API 응답시간 단축 (캐시 적중율 상승으로 2~5초 → <1초 목표【23†L289-L297】)<br/>- 운영 복잡도 감소 (디버깅 지점 단순화) |
| **DB 쿼리 최적화 및 인덱싱**<br/>(프라이머리 인덱스 튜닝, 통계 테이블 도입) | 중간 (단기) | - 확장 시 성능 저하 방지 (수백 모델 시에도 <100ms 쿼리 유지)<br/>- DB 부하 안정 (불필요한 Seq Scan 제거) |
| **Prisma ORM 사용 최적화**<br/>(필요한 데이터만 select, groupBy 활용) | 중간 (단기) | - API 서버 메모리/CPU 절약 (JS 연산 감소)<br/>- 네트워크 전송량 감소 (경량 응답)<br/>- 타임아웃 이슈 완화 (대용량 응답 제거) |
| **폴링 → 스트리밍 전환**<br/>(10초 폴링에서 SSE로 이동) | 중간 (중기) | - 평균 업데이트 지연 감소 (예: 10초 → 1-2초)<br/>- 트래픽 효율 개선 (불필요한 요청 80% 감소)<br/>- 서버리스 함수 타임아웃 문제 완화 (지속 연결로 해결) |
| **다지역 동기화 및 지역별 처리**<br/>(이벤트 전파, 지역 캐시, 지역가용성 표시) | 낮음 (중기) | - 지역 간 모델 수 불일치 거의 제거<br/>- (선행조건: 위 개선들이 적용되어야 효과 극대화)<br/>- 글로벌 서비스 확장 시 장애 예방 (일관성 문제 사전해소) |
| **장기 아키텍처 리팩토링**<br/>(마이크로서비스, GraphQL 등 도입) | 낮음 (장기) | - 코드 유지보수성 향상 및 신규 기능 확장 용이<br/>- 성능 최적화 여력 확보 (전문화된 서비스로 분리)<br/>- 단기적 성능에는 직접 영향 적으나 기술 부채 감소 |

우선 단기적으로 데이터 정확성에 치명적인 요소들(하드코딩, 이중 소스)을 제거하고, 실시간성 확보를 위한 SSE 도입과 캐시/이벤트 체계를 구축하는 데 집중해야 합니다. 이러한 1차 개선만으로도 사용자에게 일관된 **"(활성/총) 모델 수"가 100% 정확도**로 제공되고, 새 업데이트도 수 초 내 반영되는 성과를 기대할 수 있습니다. 이후에는 점진적으로 백엔드 구조를 리팩터링하고, 성능 튜닝을 반복하여 **API 응답 시간 < 1~2초, 실시간 지표 지연 < 5초, 데이터 불일치 < 5%** 수준을 달성하는 것을 목표로 합니다【23†L289-L297】.

각 개선안은 상호 보완적이며, 순차적으로 적용하면서 모니터링을 통해 효과를 검증해야 합니다. 특히 실시간화 이후에는 시스템 부하 프로파일이 달라질 수 있으므로, **모니터링 지표**(예: WebSocket 연결 수, Redis pub/sub latency, DB 쿼리 시간)를 추적하여 병목이 되는 부분을 추가 최적화하는 **반복적 개선 프로세스**가 중요합니다.

---

**참고 자료**: 본 제안은 AI-GO의 기존 아키텍처 분석【16†L13-L21】【16†L75-L83】 및 프로젝트 설계 문서들을 바탕으로 작성되었으며, 제안한 개선의 구현 세부사항은 팀의 리소스와 우선순위에 따라 조정될 수 있습니다. 향후 관련 이슈는 GitHub 저장소의 이슈 트래커에 등록하여 추적하고, 중요한 변화는 docs 폴더 내 추가 보고서로 기록할 것을 권장합니다.
